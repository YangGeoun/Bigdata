"""
<옵티마이저(Optimizer, 최적화) 기법>
 - 손실을 줄여나가기 위한 최적화(Optimizer) 방법을 의미함
 - 손실을 줄여나가는 최적화 방법으로 "경사하강법" 이론이 적용됩니다.
 - 특성들의 시작 위치에서 목적지(종속변수 위치)까지 도달하기 위한
   기울어진 방향을 찾기 위한 방법을 의미합니다.
 - "경사하강법" 이론을 적용한 여러가지 방법들중 하나를 선택하여 정의
 - 옵티마이저(최적화) 방법 
   : SGD(확률적경사하강법) < Adagrad < RMSProp < Adam, 이외 등등
 - 옵티마이저 설정 위치 : model.compile(optimizer="옵티마이저(최적화) 방법중 1개")
 
<옵티마이저(최적화) 방법 정의>
 * SGD(확률적 경사하강법)
  - 특성이 현재 위치에서 목적지까지 도달하는 과정 중에 보폭을 크게하여
    많은 길을 거치면서(극단적으로 방향을 바꿉니다.) 빠르게 탐색
  - 지그 재그 모양으로 탐색하면서 나아가는 방법
  - 아래 옵티마이저 방법들은 SGD를 근간으로 향상된 방법들 입니다.
  - 단점 : 보폭을 크게하면서 방향을 근단적으로 바꾸기 때문에 
           주변을 정밀하게 확인하기 어려움
         : 보폭을 크게하기 때문에, 목적지를 건너 띄는 경우도 발생함
                                 (종속변수를 잘 못 찾는 경우 발생)
   
 * Adagrad
  - SGD의 큰 보폭에 대한 단점을 보완한 방법...
  - 학습률(보폭)을 적절하게 설정하기 위해 학습률 감소(보폭을 짧게)라는 기술 사용
  - 학습 진행 중에 학습률을 줄여가는 방법 사용
  - 처음에는 학습률(보폭)을 크게 학습하다가,
    점점 작게(보폭을 짧게) 학습한다는 개념을 적용
  - 이미 학습된 곳은 보폭을 크게하고, 학습이 완료되었던 곳은 보폭을 짧게 세밀하게 탐색
  - 손실이 더 이상 줄어들지 않으면(손실이 0이면) 종료
  - 단점, 목적지에 도달하지 않더라도 손실이 0이면 종료하는 단점이 있음
 
 * RMSProp
  - Adagrad는 학습량을 점점 작게 학습하기 때문에
    학습률(보폭)이 0이 되어 갱신되지 않는(학습되지 않는) 시점이 발생할 수 있는 
    단점이 있음.
  - Adagrad의 단점을 보완하여,
    과거(이전)의 기울기 값을 반영하는 방식을 적용함
  - 먼 과거의 기울기(경사) 값은 조금만 반영하고,
    최근 기울기(경사)를 많이 반영하는 방식으로 처리됨
  - 과거 데이터를 저장해 놓아야 하기 때문에, 다소 훈련 시간이 걸림
  - 옵티마이저의 기본값(default)으로 사용됨, 생략가능
  
 * Adam
  - 공이 굴러가듯이 모멘텀(Momentum, 관성=방향담당)과 RMSProp을 융합한 방법
  - 방향(momentum)과 학습률(보폭)을 적절하게하여 탐색함
  - 자주 사용되는 기법으로, 좋은 결과를  얻을 수 있는 방법으로 유명함
  
 * Momentum(모멘텀)
  - 관성과 가속도를 적용하여 이동하던 방향으로 좀 더 유연하게 작동함
  - 메모리 사용이 많은 단점이 있음
    (과거 데이터를 저장해 놓고, 다음 과정에서 방향성(관성)을 이어 받아서 사용하게 됨)
"""



============================================================



"""
<학습률(Learning Rate)>
 - 경사를 내려 올 때의 "보폭"이라고 이해
 - 옵티마이저 4개는 객체(클래스)로 되어 있습니다.
 - 클래스 생성 시에 학습률(learning_rate)을 정의 할 수 있습니다.
 - 학습률이 작을  수록 -> 보폭이 작음
           높을 수록  -> 보폭이 큼
 - 가장 손실이 작은 위치를 찾아서 움직이도록 작동됩니다.
 - 이때 가장 손실이 작은 위치는 모델이 스스로 찾아주기에 사람이 관여하지 않습니다.
 - 학습률(learning_rate) 값의 범위 : 0.1~0.0001 (기본값은 0.01, 생략가능)
 - 학습률의 값에 따라서 -> 훈련 성능에 영향을 미치기에
                           => 하이퍼파라메터 대상이 됩니다.

 - 과적합을 해소하기 위한 튜닝 방법으로 사용되기도 합니다.
   * 과대적합이 발생한 경우 : 학습률을 크게(보폭을 크게) 합니다.
   * 과소적합이 발생한 경우 : 학습률을 작게(보폭을 작게) 합니다.
"""



============================================================




"""
<모멘텀(momentum)>
 - 과거에 학습된 방향(기울기)를 기억하고 있다가,
   다음의 학습 방향을 찾을 때 -> 관성을 적용시키는 방법
 - 기본적으로 0.9 이상의 값을 사용합니다.
   (기본값 = 0)
 - 보통 nesterov=True 속성과 함께 사용됩니다.
   * nesterov : 모멘텀 방향보다 조금더 앞서서 경사를 계산해 놓는 방식(미리 체크)
              : 미리 알고 있어야 방향에 관성을 적용하여 명확하게 탐색할 수 있음
 - 모멘텀 속성을 사용할 수 있는 옵티마이저 : 주로 SGD 및 Adam
"""











